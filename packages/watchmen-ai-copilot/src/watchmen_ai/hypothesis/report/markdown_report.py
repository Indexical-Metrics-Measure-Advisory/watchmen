from datetime import datetime

from watchmen_ai.hypothesis.model.data_story import DataExplain
from watchmen_ai.hypothesis.model.analysis import BusinessChallengeWithProblems, BusinessProblemWithHypotheses, \
    AnalysisMetric, HypothesisWithMetrics
from watchmen_ai.hypothesis.model.common import SimulationResult, ChallengeAnalysisResult
from watchmen_ai.markdown.document import MarkdownDocument


def build_analysis_report_md(simulation_result: SimulationResult):
    """
    åŸºäºSimulationResultåŠ¨æ€ç”Ÿæˆåˆ†ææŠ¥å‘Šçš„Markdownå†…å®¹
    å¢å¼ºç‰ˆæœ¬ï¼šåŒ…å«æ›´å¤šæ•°æ®å±•ç¤ºå’Œåˆ†æè¯¦æƒ…
    """
    md = MarkdownDocument()

    # è·å–æŒ‘æˆ˜å’Œç»“æœæ•°æ®
    challenge = BusinessChallengeWithProblems.model_validate(simulation_result.challenge)
    result = ChallengeAnalysisResult.model_validate(simulation_result.result)

    # æ·»åŠ æŠ¥å‘Šç”Ÿæˆæ—¶é—´
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    md.append_text(f"*Report generated on: {current_time}*")
    md.append_text("---")

    # 1. æŠ¥å‘Šæ ‡é¢˜å’ŒæŒ‘æˆ˜æè¿°
    md.append_heading(f"ğŸ“Œ Analysis Report: {challenge.title}", level=1)
    md.append_heading("ğŸ¯ Challenge Topic", level=2)
    md.append_text(f"### **{challenge.description}**")

    # 2. åˆ†ææ¦‚è§ˆç»Ÿè®¡
    md.append_heading("ğŸ“Š Analysis Overview", level=2)
    _add_analysis_overview(md, challenge, result, simulation_result)

    # 3. æ‰§è¡Œæ‘˜è¦ - åŸºäºchallengeInsightResult
    md.append_heading("âœ… 1ï¸âƒ£ Executive Summary", level=2)
    if result and result.challengeInsightResult:
        insight = result.challengeInsightResult
        if insight.answerForConclusion:
            md.append_text("> ### **ğŸ¯ Key Conclusion:**")
            md.append_text(f"**{insight.answerForConclusion}**")

        if insight.summaryForQuestions:
            md.append_text("### **ğŸ“‹ Summary of Key Findings:**")
            md.append_text(f"*{insight.summaryForQuestions}*")
    else:
        md.append_text("> ### **â³ Analysis in progress...** Key conclusions will be available upon completion.")

    # 3. å…³é”®æ´å¯Ÿ - åŸºäºé—®é¢˜åˆ†æç»“æœ
    md.append_heading("âœ… 2ï¸âƒ£ Key Insights", level=2)
    _add_key_insights(md, challenge, result)

    # 4. å‡è®¾åˆ†æè¯¦æƒ…
    md.append_heading("ğŸ§ª 3ï¸âƒ£ Hypothesis Analysis Details", level=2)
    _add_hypothesis_analysis(md, challenge, result)

    # 5. æ¨èçš„è¿›ä¸€æ­¥åˆ†æ
    md.append_heading("âœ… 4ï¸âƒ£ Recommended Further Analysis", level=2)
    _add_further_analysis(md, challenge, result)

    # 6. ä¸‹ä¸€æ­¥ä¸šåŠ¡è¡ŒåŠ¨
    md.append_heading("âœ… 5ï¸âƒ£ Next Business Actions", level=2)
    _add_business_actions(md, challenge, result)

    # 7. è¯„ä¼°ç»“æœ
    md.append_heading("ğŸ“‹ 6ï¸âƒ£ Evaluation Results", level=2)
    _add_evaluation_results(md, result)

    # 8. ä¸€å¥è¯æ€»ç»“
    md.append_heading("âœ… 7ï¸âƒ£ One-Sentence Summary", level=2)
    _add_summary(md, challenge, result)

    # 9. å…³é”®æ•°æ®ç‚¹
    md.append_heading("ğŸ”— 8ï¸âƒ£ Key Data Points", level=2)
    _add_key_data_points(md, challenge, result, simulation_result)

    # æ·»åŠ æŒ‡æ ‡æ˜ç»†æ•°æ®éƒ¨åˆ†ï¼ˆæ”¾åœ¨æœ€åï¼‰
    md.append_text("\n\n---\n")
    md.append_heading("ğŸ“‹ Detailed Metrics Data", level=2)
    md.append_text("\n*This section contains detailed dataset information and statistical analysis for all metrics.*\n")

    # æ·»åŠ è¯¦ç»†çš„æ•°æ®é›†ä¿¡æ¯
    if result and result.hypothesisResultDict:
        datasets_added = False
        for hypothesis_id, hypothesis_result in result.hypothesisResultDict.items():
            if hypothesis_result and hasattr(hypothesis_result,
                                             'analysis_metrics') and hypothesis_result.analysis_metrics:
                for i, metric in enumerate(hypothesis_result.analysis_metrics, 1):
                    try:
                        if isinstance(metric, dict):
                            metric = AnalysisMetric.model_validate(metric)

                        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®é›†ä¿¡æ¯
                        if hasattr(metric, 'dataset') and metric.dataset:
                            dataset_info = metric.dataset
                            if hasattr(dataset_info, 'dataset') and dataset_info.dataset:
                                dataset_obj = dataset_info.dataset
                                if hasattr(dataset_obj, '__dict__'):
                                    # è½¬æ¢ä¸ºå­—å…¸ä»¥ä¾¿å¤„ç†
                                    dataset_dict = dataset_obj.__dict__ if hasattr(dataset_obj,
                                                                                   '__dict__') else dataset_obj
                                    if not datasets_added:
                                        md.append_text("\n### ğŸ“Š Dataset Details and Statistics")
                                        datasets_added = True
                                    _add_detailed_dataset_table(md, dataset_dict, metric.name or f"Metric {i}")
                                elif isinstance(dataset_obj, dict):
                                    if not datasets_added:
                                        md.append_text("\n### ğŸ“Š Dataset Details and Statistics")
                                        datasets_added = True
                                    _add_detailed_dataset_table(md, dataset_obj, metric.name or f"Metric {i}")
                    except Exception as e:
                        continue

        if not datasets_added:
            md.append_text("\n*No detailed dataset information available for metrics analysis.*")
    else:
        md.append_text("\n*No metrics data available for detailed analysis.*")

    return md.contents()


def _add_analysis_overview(md: MarkdownDocument, challenge: BusinessChallengeWithProblems,
                           result: ChallengeAnalysisResult, simulation_result: SimulationResult):
    """æ·»åŠ åˆ†ææ¦‚è§ˆç»Ÿè®¡"""
    total_problems = len(challenge.problems) if challenge.problems else 0
    completed_problems = 0
    total_hypotheses = 0
    completed_hypotheses = 0

    # ç»Ÿè®¡å·²å®Œæˆçš„é—®é¢˜åˆ†æ
    if result and result.questionResultDict:
        for problem in challenge.problems or []:
            problem = BusinessProblemWithHypotheses.model_validate(problem)
            question_result = result.questionResultDict.get(problem.id)
            if question_result and hasattr(question_result,
                                           'answerForConclusion') and question_result.answerForConclusion:
                completed_problems += 1

    # ç»Ÿè®¡å‡è®¾åˆ†æ
    if result and result.hypothesisResultDict:
        total_hypotheses = len(result.hypothesisResultDict)
        for hypothesis_result in result.hypothesisResultDict.values():
            if hypothesis_result and hasattr(hypothesis_result,
                                             'answerForConclusion') and hypothesis_result.answerForConclusion:
                completed_hypotheses += 1

    # åˆ›å»ºæ¦‚è§ˆè¡¨æ ¼
    overview_headers = ["Metric", "Count", "Status"]
    overview_rows = [
        ["Total Problems", str(total_problems), f"{completed_problems}/{total_problems} Analyzed"],
        ["Total Hypotheses", str(total_hypotheses), f"{completed_hypotheses}/{total_hypotheses} Analyzed"],
        ["Challenge Title", challenge.title, "ğŸ“‹ Active"],
        ["Analysis Status", getattr(simulation_result, 'environmentStatus', 'In Progress'), "ğŸ”„ Processing"]
    ]

    md.append_table(overview_headers, overview_rows)


def _add_key_insights(md: MarkdownDocument, challenge: BusinessChallengeWithProblems, result: ChallengeAnalysisResult):
    """æ·»åŠ å…³é”®æ´å¯Ÿ"""
    if result and result.questionResultDict:
        insights_found = False
        
        for problem in challenge.problems or []:
            problem = BusinessProblemWithHypotheses.model_validate(problem)
            question_result = result.questionResultDict.get(problem.id)
            
            if question_result and hasattr(question_result, 'answerForConclusion') and question_result.answerForConclusion:
                insights_found = True
                md.append_text(f"**ğŸ“‹ {problem.title}**")
                md.append_text(f"> âœ… **Status:** Completed")
                md.append_text(f"> **Key Insight:** {question_result.answerForConclusion}")
                md.append_text("")
            else:
                insights_found = True
                md.append_text(f"**ğŸ“‹ {problem.title}**")
                md.append_text(f"> â³ **Status:** Pending")
                md.append_text(f"> **Key Insight:** Analysis in progress...")
                md.append_text("")

        if not insights_found:
            md.append_text("*Key insights are being generated based on question analysis results.*")
    else:
        md.append_text("*Key insights are being generated based on hypothesis analysis results.*")


def _add_hypothesis_analysis(md: MarkdownDocument, challenge: BusinessChallengeWithProblems,
                             result: ChallengeAnalysisResult):
    """æ·»åŠ å‡è®¾åˆ†æè¯¦æƒ…"""
    if result and result.hypothesisResultDict:
        analysis_found = False
        
        for problem in challenge.problems or []:
            problem = BusinessProblemWithHypotheses.model_validate(problem)
            
            # æ˜¾ç¤ºè¯¥é—®é¢˜ä¸‹çš„å‡è®¾åˆ†æ
            for hypothesis in problem.hypotheses or []:
                hypothesis = HypothesisWithMetrics.model_validate(hypothesis)
                hypothesis_result = result.hypothesisResultDict.get(hypothesis.id)
                analysis_found = True
                
                md.append_text(f"**ğŸ§ª Problem:** {problem.title}")
                md.append_text(f"**ğŸ“‹ Hypothesis:** {hypothesis.description}")
                
                if hypothesis_result:
                    # æå–ç»“è®ºæ–‡æœ¬
                    if hasattr(hypothesis_result, 'answerForConclusion') and hypothesis_result.answerForConclusion:
                        result_text = hypothesis_result.answerForConclusion
                    else:
                        result_text = "Analysis completed"
                    
                    # æå–AnalysisDataä¿¡æ¯
                    metrics_count = "0"
                    data_points = "0"
                    
                    if hasattr(hypothesis_result, 'analysis_metrics') and hypothesis_result.analysis_metrics:
                        metrics_count = str(len(hypothesis_result.analysis_metrics))
                    
                    if hasattr(hypothesis_result, 'data_explain_dict') and hypothesis_result.data_explain_dict:
                        data_points = str(len(hypothesis_result.data_explain_dict))
                    
                    md.append_text(f"> âœ… **Status:** Completed")
                    md.append_text(f"> **Analysis Result:** {result_text}")
                    md.append_text(f"> **Metrics Count:** {metrics_count}")
                    md.append_text(f"> **Data Points:** {data_points}")
                else:
                    md.append_text(f"> â³ **Status:** Pending")
                    md.append_text(f"> **Analysis Result:** Analysis in progress...")
                    md.append_text(f"> **Metrics Count:** N/A")
                    md.append_text(f"> **Data Points:** N/A")
                
                md.append_text("")
        
        if analysis_found:
            # æ·»åŠ è¯¦ç»†çš„AnalysisDataå†…å®¹
            md.append_text("\n**Detailed Analysis Data:**")
            _add_detailed_analysis_data(md, result, challenge)
        else:
            md.append_text("*No hypothesis analysis data available.*")
    else:
        md.append_text("*No hypothesis analysis data available.*")


def _add_further_analysis(md: MarkdownDocument, challenge: BusinessChallengeWithProblems,
                          result: ChallengeAnalysisResult):
    """æ·»åŠ æ¨èçš„è¿›ä¸€æ­¥åˆ†æ"""
    if result and result.challengeInsightResult:
        # ä¼˜å…ˆä½¿ç”¨futureAnalysisForConclusion
        future_analysis = getattr(result.challengeInsightResult, 'futureAnalysisForConclusion', None) or \
                          getattr(result.challengeInsightResult, 'futureAnalysis', None)

        if future_analysis:
            md.append_text("### **ğŸ” Recommended Next Steps for Analysis:**")
            # å°†åˆ†æå»ºè®®æŒ‰è¡Œåˆ†å‰²å¹¶æ ¼å¼åŒ–
            analysis_lines = future_analysis.split('\n')
            for i, line in enumerate(analysis_lines, 1):
                line = line.strip()
                if line:
                    # ç§»é™¤å¯èƒ½çš„bullet pointç¬¦å·
                    if line.startswith('-') or line.startswith('â€¢'):
                        line = line[1:].strip()
                    md.append_text(f"**{i}.** ***{line}***")
        else:
            md.append_text("*Further analysis recommendations will be generated upon completion.*")
    else:
        # æä¾›é»˜è®¤çš„æ¨èåˆ†æ
        md.append_text("**Based on current analysis, we recommend:**")
        md.append_text("**1.** Review and validate key findings with stakeholders.")
        md.append_text("**2.** Gather additional data to support key findings.")
        md.append_text("**3.** Conduct deeper analysis on high-impact metrics.")


def _add_business_actions(md: MarkdownDocument, challenge: BusinessChallengeWithProblems,
                          result: ChallengeAnalysisResult):
    """æ·»åŠ ä¸‹ä¸€æ­¥ä¸šåŠ¡è¡ŒåŠ¨"""
    if result and result.challengeInsightResult:
        # ä¼˜å…ˆä½¿ç”¨futureBusinessActionForConclusion
        business_action = getattr(result.challengeInsightResult, 'futureBusinessActionForConclusion', None) or \
                          getattr(result.challengeInsightResult, 'futureBusinessAction', None)

        if business_action:
            md.append_text("### **ğŸš€ Recommended Business Actions:**")
            # å°†ä¸šåŠ¡è¡ŒåŠ¨æŒ‰è¡Œåˆ†å‰²å¹¶æ ¼å¼åŒ–
            business_actions = business_action.split('\n')
            action_count = 1
            for action in business_actions:
                action = action.strip()
                if action:
                    # ç§»é™¤å¯èƒ½çš„bullet pointç¬¦å·
                    if action.startswith('-') or action.startswith('â€¢'):
                        action = action[1:].strip()
                    md.append_text(f"## **{action_count}ï¸âƒ£ {action}**")
                    action_count += 1
        else:
            md.append_text("*Business action recommendations will be generated upon completion.*")
    else:
        # æä¾›é»˜è®¤çš„ä¸šåŠ¡è¡ŒåŠ¨
        md.append_text("### **ğŸš€ Recommended Business Actions:**")
        md.append_text("## **1ï¸âƒ£ Implement Data-Driven Strategies**")
        md.append_text("   â€¢ ***Apply insights from completed analysis to business operations.***")
        md.append_text("## **2ï¸âƒ£ Monitor Key Metrics**")
        md.append_text("   â€¢ ***Establish tracking mechanisms for identified success indicators.***")
        md.append_text("## **3ï¸âƒ£ Iterate and Improve**")
        md.append_text("   â€¢ ***Continuously refine strategies based on performance data.***")


def _add_evaluation_results(md: MarkdownDocument, result: ChallengeAnalysisResult):
    """æ·»åŠ è¯„ä¼°ç»“æœ"""
    if result and hasattr(result, 'evaluation') and result.evaluation:
        md.append_text("### **ğŸ“Š Overall Analysis Evaluation:**")

        # åˆ›å»ºè¯„ä¼°ç»“æœè¡¨æ ¼
        eval_headers = ["Dimension", "Score", "Comments"]
        eval_rows = []

        evaluation = result.evaluation
        if hasattr(evaluation, 'goal_alignment') and evaluation.goal_alignment:
            score = f"{evaluation.goal_alignment_score}/5" if hasattr(evaluation,
                                                                      'goal_alignment_score') and evaluation.goal_alignment_score is not None else "N/A"
            eval_rows.append(["Goal Alignment", score, evaluation.goal_alignment])

        if hasattr(evaluation, 'challenge_understanding') and evaluation.challenge_understanding:
            score = f"{evaluation.challenge_understanding_score}/5" if hasattr(evaluation,
                                                                               'challenge_understanding_score') and evaluation.challenge_understanding_score is not None else "N/A"
            eval_rows.append(["Challenge Understanding", score, evaluation.challenge_understanding])

        if hasattr(evaluation, 'hypothesis_coverage') and evaluation.hypothesis_coverage:
            score = f"{evaluation.hypothesis_coverage_score}/5" if hasattr(evaluation,
                                                                           'hypothesis_coverage_score') and evaluation.hypothesis_coverage_score is not None else "N/A"
            eval_rows.append(["Hypothesis Coverage", score, evaluation.hypothesis_coverage])

        if hasattr(evaluation, 'actionable_insights') and evaluation.actionable_insights:
            score = f"{evaluation.actionable_insights_score}/5" if hasattr(evaluation,
                                                                           'actionable_insights_score') and evaluation.actionable_insights_score is not None else "N/A"
            eval_rows.append(["Actionable Insights", score, evaluation.actionable_insights])

        if hasattr(evaluation, 'verification_reliability') and evaluation.verification_reliability:
            score = f"{evaluation.verification_reliability_score}/5" if hasattr(evaluation,
                                                                                'verification_reliability_score') and evaluation.verification_reliability_score is not None else "N/A"
            eval_rows.append(["Verification Reliability", score, evaluation.verification_reliability])

        if hasattr(evaluation, 'data_sufficiency') and evaluation.data_sufficiency:
            score = f"{evaluation.data_sufficiency_score}/5" if hasattr(evaluation,
                                                                        'data_sufficiency_score') and evaluation.data_sufficiency_score is not None else "N/A"
            eval_rows.append(["Data Sufficiency", score, evaluation.data_sufficiency])

        if eval_rows:
            md.append_table(eval_headers, eval_rows)
        else:
            md.append_text("*Evaluation data is being processed...*")
    elif result and result.hypothesisResultDict:
        evaluation_headers = ["Hypothesis ID", "Analysis Status", "Completion"]
        evaluation_rows = []

        for hypothesis_id, hypothesis_result in result.hypothesisResultDict.items():
            if hypothesis_result:
                has_conclusion = hasattr(hypothesis_result,
                                         'answerForConclusion') and hypothesis_result.answerForConclusion
                status = "âœ… Completed" if has_conclusion else "â³ In Progress"
                completion = "100%" if has_conclusion else "Pending"
                evaluation_rows.append(
                    [hypothesis_id[:20] + "..." if len(hypothesis_id) > 20 else hypothesis_id, status, completion])

        if evaluation_rows:
            md.append_table(evaluation_headers, evaluation_rows)
        else:
            md.append_text("*Evaluation results will be available upon completion of hypothesis analysis.*")
    else:
        md.append_text("*Evaluation results are being generated...*")


def _add_summary(md: MarkdownDocument, challenge: BusinessChallengeWithProblems, result: ChallengeAnalysisResult):
    """æ·»åŠ ä¸€å¥è¯æ€»ç»“"""
    if result and result.challengeInsightResult and result.challengeInsightResult.answerForConclusion:
        # æå–ç»“è®ºçš„ç¬¬ä¸€å¥ä½œä¸ºæ€»ç»“
        conclusion = result.challengeInsightResult.answerForConclusion
        summary = conclusion.split('.')[0] + '.' if '.' in conclusion else conclusion
        md.append_text(f"> ## **ğŸ¯ Key Takeaway:** ***{summary}***")
    else:
        md.append_text(
            f"> ## **ğŸ¯ Key Takeaway:** ***{challenge.title} analysis provides actionable insights for data-driven business decisions.***")


def _add_detailed_analysis_data(md: MarkdownDocument, result: ChallengeAnalysisResult,
                                challenge: BusinessChallengeWithProblems = None):
    """æ·»åŠ è¯¦ç»†çš„AnalysisDataå†…å®¹"""
    if not result or not result.hypothesisResultDict:
        md.append_text("*No detailed analysis data available.*")
        return

    # Create a mapping from hypothesis_id to hypothesis title
    hypothesis_id_to_title = {}
    if challenge and challenge.problems:
        for problem in challenge.problems:
            problem = BusinessProblemWithHypotheses.model_validate(problem)
            for hypothesis in problem.hypotheses or []:
                hypothesis = HypothesisWithMetrics.model_validate(hypothesis)
                hypothesis_id_to_title[hypothesis.id] = hypothesis.title

    analysis_data_found = False

    for hypothesis_id, hypothesis_result in result.hypothesisResultDict.items():
        if hypothesis_result and (
                hasattr(hypothesis_result, 'analysis_metrics') or hasattr(hypothesis_result, 'data_explain_dict')):
            analysis_data_found = True

            # æ·»åŠ å‡è®¾æ ‡é¢˜ä½œä¸ºå­æ ‡é¢˜
            hypothesis_title = hypothesis_id_to_title.get(hypothesis_id, hypothesis_id)
            display_title = hypothesis_title
            md.append_text(f"\n### **ğŸ§ª Hypothesis: {display_title}**")

            

            # æ˜¾ç¤ºæ•°æ®è§£é‡Š
            if hasattr(hypothesis_result, 'data_explain_dict') and hypothesis_result.data_explain_dict:
                md.append_text("\n### ğŸ“‹ Data Explanations")
                md.append_text("")

                # å¦‚æœåªæœ‰ä¸€ä¸ªè§£é‡Šï¼Œç›´æ¥æ˜¾ç¤ºæ–‡æœ¬æ ¼å¼
                if len(hypothesis_result.data_explain_dict) == 1:
                    explain = hypothesis_result.data_explain_dict[0]
                    try:
                        if isinstance(explain, dict):
                            explain = DataExplain.model_validate(explain)

                        # è·å–å‡è®¾éªŒè¯ä¿¡æ¯
                        validation = explain.hypothesisValidation
                        validation_flag = explain.hypothesisValidationFlag

                        # Build validation status display with better formatting
                        md.append_text("**ğŸ” Validation Status:**")
                        if validation_flag is not None:
                            status_icon = "âœ…" if validation_flag else "âŒ"
                            md.append_text(f"> {status_icon} ***{validation}***")
                        else:
                            md.append_text(f"> â³ ***{validation}***")
                        md.append_text("")

                        # Get key metric changes with better formatting
                        key_metric = explain.keyMetricChange
                        md.append_text("**ğŸ“Š Key Metric Changes:**")
                        md.append_text(f"> ***{key_metric}***")
                        md.append_text("")

                        # Get summary findings with better formatting
                        summary = explain.summaryFinding
                        md.append_text("**ğŸ“ Summary Findings:**")
                        md.append_text(f"> ***{summary}***")
                        md.append_text("")

                    except Exception as e:
                        md.append_text(f"âŒ Error parsing explanation: {str(e)}")

                # å¦‚æœæœ‰å¤šä¸ªè§£é‡Šï¼Œä½¿ç”¨æ”¹è¿›çš„æ˜¾ç¤ºæ ¼å¼
                else:
                    for i, explain in enumerate(hypothesis_result.data_explain_dict, 1):
                        try:
                            if isinstance(explain, dict):
                                explain = DataExplain.model_validate(explain)

                            md.append_text(f"#### ğŸ“„ Explanation {i}")
                            md.append_text("")

                            # è·å–å‡è®¾éªŒè¯ä¿¡æ¯
                            validation = explain.hypothesisValidation
                            validation_flag = explain.hypothesisValidationFlag

                            # æ„å»ºéªŒè¯çŠ¶æ€æ˜¾ç¤º
                            md.append_text("**ğŸ” Validation Status:**")
                            if validation_flag is not None:
                                status_icon = "âœ…" if validation_flag else "âŒ"
                                md.append_text(f"> {status_icon} {validation}")
                            else:
                                md.append_text(f"> â³ {validation}")
                            md.append_text("")

                            # è·å–å…³é”®æŒ‡æ ‡å˜åŒ–
                            key_metric = explain.keyMetricChange
                            md.append_text("**ğŸ“Š Key Metric Changes:**")
                            md.append_text(f"> {key_metric}")
                            md.append_text("")

                            # è·å–æ€»ç»“å‘ç°
                            summary = explain.summaryFinding
                            md.append_text("**ğŸ“ Summary Findings:**")
                            md.append_text(f"> {summary}")

                            # æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
                            if i < len(hypothesis_result.data_explain_dict):
                                md.append_text("")
                                md.append_text("---")
                                md.append_text("")

                        except Exception as e:
                            md.append_text(f"#### âŒ Explanation {i} - Error")
                            md.append_text(f"> Error parsing explanation: {str(e)}")
                            md.append_text("")

            # æ˜¾ç¤ºåˆ†ææŒ‡æ ‡
            if hasattr(hypothesis_result, 'analysis_metrics') and hypothesis_result.analysis_metrics:
                md.append_text("\n#### **ğŸ“Š Analysis Metrics:**")
                metrics_headers = ["Metric Name", "Dimensions", "Dataset"]
                metrics_rows = []

                for metric in hypothesis_result.analysis_metrics:
                    try:
                        # éªŒè¯å¹¶è½¬æ¢metricå¯¹è±¡
                        if isinstance(metric, dict):
                            metric = AnalysisMetric.model_validate(metric)

                        metric_name = getattr(metric, 'name', 'Unknown Metric') or 'Unknown Metric'
                        metric_category = getattr(metric, 'category', 'General') or 'General'

                        # æ„å»ºæ˜¾ç¤ºåç§°
                        display_name = f"{metric_name} ({metric_category})" if metric_name != 'Unknown Metric' else metric_category

                        # æå–æ•°æ®é›†ä¿¡æ¯
                        dataset_info = "No dataset"
                        if hasattr(metric, 'dataset') and metric.dataset:
                            dataset = metric.dataset
                            if hasattr(dataset, 'dataset') and dataset.dataset:
                                # æ£€æŸ¥æ•°æ®é›†ç»“æ„
                                dataset_obj = dataset.dataset
                                if isinstance(dataset_obj, dict):
                                    # å¦‚æœæœ‰æ•°æ®ï¼Œæ˜¾ç¤ºæ•°æ®è¡Œæ•°å’Œåˆ—ä¿¡æ¯
                                    if 'data' in dataset_obj and 'column_names' in dataset_obj:
                                        data_rows = len(dataset_obj['data']) if dataset_obj['data'] else 0
                                        columns = len(dataset_obj['column_names']) if dataset_obj['column_names'] else 0
                                        dataset_info = f"{data_rows} rows, {columns} cols"
                                    else:
                                        dataset_info = "Dataset structure available"
                                else:
                                    # å°è¯•è·å–æ•°æ®é›†åç§°æˆ–ID
                                    dataset_name = getattr(dataset_obj, 'name', None) or getattr(dataset_obj, 'id',
                                                                                                 'Dataset available')
                                    dataset_info = str(dataset_name)[:30] + "..." if len(
                                        str(dataset_name)) > 30 else str(dataset_name)
                            else:
                                dataset_info = "Dataset configured"

                        # æå–ç»´åº¦ä¿¡æ¯
                        dimensions_info = "No dimensions"
                        if hasattr(metric, 'dimensions') and metric.dimensions:
                            dim_names = []
                            for dim in metric.dimensions:
                                if isinstance(dim, dict):
                                    dim_name = dim.get('name', 'Unknown')
                                    dim_type = dim.get('dimensionType', '')
                                    if dim_type:
                                        dim_names.append(f"{dim_name}({dim_type})")
                                    else:
                                        dim_names.append(dim_name)
                                else:
                                    dim_name = getattr(dim, 'name', 'Unknown')
                                    dim_type = getattr(dim, 'dimensionType', '')
                                    if dim_type:
                                        dim_names.append(f"{dim_name}({dim_type})")
                                    else:
                                        dim_names.append(dim_name)

                            if dim_names:
                                dimensions_info = ", ".join(dim_names[:3])  # é™åˆ¶æ˜¾ç¤ºå‰3ä¸ªç»´åº¦
                                if len(metric.dimensions) > 3:
                                    dimensions_info += f" (+{len(metric.dimensions) - 3} more)"
                            else:
                                dimensions_info = f"{len(metric.dimensions)} dimension(s)"

                        metrics_rows.append([display_name, dimensions_info, dataset_info])
                    except Exception as e:
                        # å¤„ç†å¼‚å¸¸æƒ…å†µ
                        metrics_rows.append(["Error parsing metric", "N/A", str(e)[:30] + "..."])

                if metrics_rows:
                    md.append_table(metrics_headers, metrics_rows)
                else:
                    md.append_text("  â€¢ No detailed metrics available")
            # æ·»åŠ åˆ†éš”çº¿
            md.append_text("---")

    if not analysis_data_found:
        md.append_text("*No detailed analysis metrics or data explanations found in hypothesis results.*")


# ===== æ•°æ®è¯¦æƒ…å’Œç»Ÿè®¡å‡½æ•° =====

# ===== æŒ‡æ ‡æ˜ç»†æ•°æ®å‡½æ•° =====

def _add_detailed_dataset_table(md: MarkdownDocument, dataset_obj: dict, metric_name: str):
    """æ·»åŠ è¯¦ç»†çš„æ•°æ®é›†è¡¨æ ¼å’Œç»Ÿè®¡æ‘˜è¦"""
    if not dataset_obj or 'data' not in dataset_obj or 'column_names' not in dataset_obj:
        return

    data = dataset_obj['data']
    column_names = dataset_obj['column_names']

    if not data or not column_names:
        return

    md.append_text(f"\n**ğŸ“Š Dataset Details for {metric_name}:**")

    # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼ï¼ˆé™åˆ¶æ˜¾ç¤ºå‰20è¡Œä»¥é¿å…è¿‡é•¿ï¼‰
    display_rows = data[:20] if len(data) > 20 else data
    md.append_table(column_names, display_rows)

    if len(data) > 20:
        md.append_text(f"*Showing first 20 rows of {len(data)} total rows...*")

    # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
    md.append_text("\n**ğŸ“ˆ Statistical Summary:**")
    stats_headers = ["Column", "Data Type", "Unique Values", "Sample Values"]
    stats_rows = []

    for i, col_name in enumerate(column_names):
        # æå–è¯¥åˆ—çš„æ‰€æœ‰å€¼
        col_values = [row[i] if i < len(row) else None for row in data if row]
        col_values = [v for v in col_values if v is not None]  # è¿‡æ»¤Noneå€¼

        if not col_values:
            stats_rows.append([col_name, "No Data", "0", "N/A"])
            continue

        # åˆ¤æ–­æ•°æ®ç±»å‹
        sample_value = col_values[0]
        if isinstance(sample_value, (int, float)):
            data_type = "Numeric"
            unique_count = len(set(col_values))
            # å¯¹äºæ•°å€¼å‹æ•°æ®ï¼Œæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            try:
                min_val = min(col_values)
                max_val = max(col_values)
                avg_val = sum(col_values) / len(col_values)
                sample_info = f"Min: {min_val}, Max: {max_val}, Avg: {avg_val:.2f}"
            except:
                sample_info = str(col_values[:3])[:30] + "..."
        elif isinstance(sample_value, str) and 'T' in sample_value and ':' in sample_value:
            data_type = "DateTime"
            unique_count = len(set(col_values))
            # å¯¹äºæ—¥æœŸæ—¶é—´ï¼Œæ˜¾ç¤ºèŒƒå›´
            try:
                sorted_dates = sorted(set(col_values))
                sample_info = f"From: {sorted_dates[0][:10]} To: {sorted_dates[-1][:10]}"
            except:
                sample_info = str(col_values[:2])[:30] + "..."
        else:
            data_type = "Text/Other"
            unique_count = len(set(str(v) for v in col_values))
            # æ˜¾ç¤ºå‰å‡ ä¸ªå”¯ä¸€å€¼
            unique_vals = list(set(str(v) for v in col_values))[:3]
            sample_info = ", ".join(unique_vals)[:30] + ("..." if len(unique_vals) > 3 else "")

        stats_rows.append([col_name, data_type, str(unique_count), sample_info])

    md.append_table(stats_headers, stats_rows)

    # æ·»åŠ æ€»ä½“ç»Ÿè®¡
    md.append_text("\n**ğŸ“‹ Dataset Overview:**")
    overview_data = [
        ["Total Rows", str(len(data))],
        ["Total Columns", str(len(column_names))],
        ["Data Density",
         f"{(sum(1 for row in data for cell in row if cell is not None) / (len(data) * len(column_names)) * 100):.1f}%" if data and column_names else "0%"]
    ]
    md.append_table(["Metric", "Value"], overview_data)
    md.append_text("\n---\n")


def _add_dataset_details(md: MarkdownDocument, result: ChallengeAnalysisResult):
    """æ·»åŠ æ•°æ®é›†è¯¦ç»†ä¿¡æ¯"""
    if not result or not result.hypothesisResultDict:
        md.append_text("*No dataset information available.*")
        return

    datasets_found = False

    for hypothesis_id, hypothesis_result in result.hypothesisResultDict.items():
        if hypothesis_result and hasattr(hypothesis_result, 'analysis_metrics'):
            for metric in hypothesis_result.analysis_metrics:
                try:
                    if isinstance(metric, dict):
                        metric = AnalysisMetric.model_validate(metric)

                    if hasattr(metric, 'dataset') and metric.dataset and hasattr(metric.dataset, 'dataset'):
                        dataset = metric.dataset.dataset
                        if not datasets_found:
                            datasets_found = True

                        # è·å–æ•°æ®é›†åç§°
                        dataset_name = getattr(dataset, 'name', None) or getattr(dataset, 'id',
                                                                                 f"Dataset for {hypothesis_id[:20]}")
                        md.append_text(f"\n**Dataset: {dataset_name}**")

                        # æå–æ•°æ®é›†å±æ€§
                        dataset_headers = ["Property", "Value"]
                        dataset_rows = []

                        # å¸¸è§çš„æ•°æ®é›†å±æ€§
                        properties = ['source', 'rows_count', 'columns_count', 'time_range', 'description']
                        for prop in properties:
                            if hasattr(dataset, prop):
                                value = getattr(dataset, prop)
                                if value is not None:
                                    # æ ¼å¼åŒ–æ˜¾ç¤ºå€¼
                                    if isinstance(value, (int, float)):
                                        display_value = f"{value:,}"
                                    elif isinstance(value, str) and len(value) > 50:
                                        display_value = value[:50] + "..."
                                    else:
                                        display_value = str(value)
                                    dataset_rows.append([prop.replace('_', ' ').title(), display_value])

                        if dataset_rows:
                            md.append_table(dataset_headers, dataset_rows)
                        else:
                            md.append_text("  â€¢ No detailed dataset properties available")
                except Exception as e:
                    continue

    if not datasets_found:
        md.append_text("*No detailed dataset information found in analysis metrics.*")


def _add_key_data_points(md: MarkdownDocument, challenge: BusinessChallengeWithProblems,
                         result: ChallengeAnalysisResult, simulation_result: SimulationResult):
    """æ·»åŠ å…³é”®æ•°æ®ç‚¹"""
    # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
    md.append_text("**ğŸ“‹ Basic Information:**")
    md.append_text(f"> **Challenge Title:** {challenge.title}")
    md.append_text(f"> **Total Problems:** {len(challenge.problems) if challenge.problems else 0}")
    md.append_text(f"> **Analysis Status:** {getattr(simulation_result, 'environmentStatus', 'In Progress')}")
    md.append_text("")

    # ä»hypothesisç»“æœä¸­æå–å…³é”®æŒ‡æ ‡
    if result and result.hypothesisResultDict:
        completed_hypotheses = 0
        total_metrics = 0
        total_data_points = 0

        for hypothesis_id, hypothesis_result in result.hypothesisResultDict.items():
            if hypothesis_result and hasattr(hypothesis_result,
                                             'answerForConclusion') and hypothesis_result.answerForConclusion:
                completed_hypotheses += 1

            # ç»Ÿè®¡AnalysisDataå†…å®¹
            if hypothesis_result:
                if hasattr(hypothesis_result, 'analysis_metrics') and hypothesis_result.analysis_metrics:
                    total_metrics += len(hypothesis_result.analysis_metrics)
                if hasattr(hypothesis_result, 'data_explain_dict') and hypothesis_result.data_explain_dict:
                    total_data_points += len(hypothesis_result.data_explain_dict)

        # æ·»åŠ å®Œæˆç‡
        completion_rate = f"{(completed_hypotheses / len(result.hypothesisResultDict) * 100):.1f}%" if result.hypothesisResultDict else "0%"
        
        md.append_text("**ğŸ“Š Analysis Statistics:**")
        md.append_text(f"> **Total Hypotheses:** {len(result.hypothesisResultDict)}")
        md.append_text(f"> **Completed Hypotheses:** {completed_hypotheses}")
        md.append_text(f"> **Completion Rate:** {completion_rate}")
        md.append_text(f"> **Total Analysis Metrics:** {total_metrics}")
        md.append_text(f"> **Total Data Explanations:** {total_data_points}")
        md.append_text("")

    # æ·»åŠ æ—¶é—´ä¿¡æ¯
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    md.append_text("**â° Report Information:**")
    md.append_text(f"> **Report Generated:** {current_time}")


