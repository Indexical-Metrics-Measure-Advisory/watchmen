from typing import List, Dict, Any, Optional

from watchmen_ai.hypothesis.agent.types import IntentType
from watchmen_ai.hypothesis.rag.rag_emb_service import (
    get_rag_embedding_service, 
    search_similar_content, 
    search_semantic_content,
    VectorSearchResult
)
from watchmen_utilities import ExtendedBaseModel


class RAGAnalyzer(ExtendedBaseModel):
    """å¢å¼ºçš„RAGåˆ†æå™¨ï¼Œæ”¯æŒè¯­ä¹‰æœç´¢å’Œæ™ºèƒ½æ–‡æ¡£æ£€ç´¢"""

    def __init__(self):
        super().__init__()
        self.rag_service = get_rag_embedding_service()

    async def retrieve_relevant_docs(self, query: str, intent: IntentType, 
                                    semantic_sections: Optional[List[str]] = None,
                                    content_types: Optional[List[str]] = None,
                                    limit: int = 10) -> List[Dict[str, Any]]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œæ”¯æŒè¯­ä¹‰æ®µè½å’Œå†…å®¹ç±»å‹è¿‡æ»¤
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            intent: æ„å›¾ç±»å‹
            semantic_sections: è¯­ä¹‰æ®µè½ç±»å‹è¿‡æ»¤ ['hypothesis', 'conclusion', 'recommendation', 'analysis', 'executive_summary', 'future_action']
            content_types: å†…å®¹ç±»å‹è¿‡æ»¤ ['full_report', 'executive_summary', 'hypothesis_analysis']
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
        """
        try:
            # æ ¹æ®æ„å›¾ç±»å‹é€‰æ‹©åˆé€‚çš„è¯­ä¹‰æ®µè½
            if semantic_sections is None:
                semantic_sections = self._get_semantic_sections_by_intent(intent)
            
            # ä½¿ç”¨è¯­ä¹‰æœç´¢è·å–æ›´ç²¾å‡†çš„ç»“æœ
            if semantic_sections:
                results = await search_semantic_content(
                    query=query,
                    semantic_sections=semantic_sections,
                    limit=limit
                )
            else:
                # å›é€€åˆ°æ™®é€šç›¸ä¼¼æ€§æœç´¢
                results = await self.rag_service.search_similar_challenges(
                    query=query,
                    limit=limit,
                    content_types=content_types
                )
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼å¹¶å¢å¼ºå…ƒæ•°æ®
            converted_results = []
            for result in results:
                doc_dict = {
                    "title": result.document.challenge_title,
                    "content": result.document.markdown_content,
                    "score": result.score,
                    "distance": result.distance,
                    "document_id": result.document.id,
                    "content_type": result.document.content_type,
                    "simulation_id": result.document.simulation_id,
                    "challenge_description": result.document.challenge_description,
                    # æ–°å¢çš„è¯­ä¹‰ä¿¡æ¯
                    "semantic_section": result.document.semantic_section,
                    "section_title": result.document.section_title,
                    "token_count": result.document.token_count,
                    "chunk_index": result.document.chunk_index,
                    "total_chunks": result.document.total_chunks,
                    "metadata": result.document.metadata
                }
                converted_results.append(doc_dict)
            
            return converted_results
            
        except Exception as e:
            # å¦‚æœè¯­ä¹‰æœç´¢å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€æœç´¢
            results = await search_similar_content(query, limit)
            converted_results = []
            for result in results:
                converted_results.append({
                    "title": result.document.challenge_title,
                    "content": result.document.markdown_content,
                    "score": result.score,
                    "distance": result.distance,
                    "document_id": result.document.id,
                    "content_type": result.document.content_type,
                    "simulation_id": result.document.simulation_id,
                    "challenge_description": result.document.challenge_description,
                    "semantic_section": getattr(result.document, 'semantic_section', 'general'),
                    "section_title": getattr(result.document, 'section_title', ''),
                    "token_count": getattr(result.document, 'token_count', 0),
                    "chunk_index": result.document.chunk_index,
                    "total_chunks": result.document.total_chunks,
                    "metadata": getattr(result.document, 'metadata', {})
                })
            return converted_results


    async def generate_analysis_report(self, rag_results: List[Dict[str, Any]],
                                     user_input: str, metrics_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆå¢å¼ºçš„åˆ†ææŠ¥å‘Šï¼ŒåŸºäºè¯­ä¹‰æ®µè½å’Œæ™ºèƒ½å†…å®¹ç»„ç»‡"""
        report_sections = []

        # æ·»åŠ æ‰§è¡Œæ‘˜è¦
        report_sections.append("# æ™ºèƒ½åˆ†ææŠ¥å‘Š")
        report_sections.append(f"**æŸ¥è¯¢å†…å®¹**: {user_input}")
        report_sections.append(f"**ç”Ÿæˆæ—¶é—´**: {self._get_current_time()}")
        report_sections.append(f"**æ•°æ®æ¥æº**: {len(rag_results)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        # æŒ‰è¯­ä¹‰æ®µè½ç±»å‹ç»„ç»‡å‘ç°
        if rag_results:
            semantic_findings = self._organize_findings_by_semantic_type(rag_results)
            
            # å…³é”®å‡è®¾
            if 'hypothesis' in semantic_findings:
                report_sections.append("\n## ğŸ” å…³é”®å‡è®¾åˆ†æ")
                for finding in semantic_findings['hypothesis']:
                    report_sections.append(f"### {finding['title']}")
                    report_sections.append(f"**ç›¸å…³åº¦**: {finding['score']:.3f}")
                    report_sections.append(f"**å†…å®¹æ‘˜è¦**: {finding['content'][:200]}...")
                    if finding['section_title']:
                        report_sections.append(f"**æ®µè½**: {finding['section_title']}")
                    report_sections.append("")
            
            # ç»“è®ºæ€»ç»“
            if 'conclusion' in semantic_findings:
                report_sections.append("\n## ğŸ“Š ç»“è®ºæ€»ç»“")
                for finding in semantic_findings['conclusion']:
                    report_sections.append(f"### {finding['title']}")
                    report_sections.append(f"**ç›¸å…³åº¦**: {finding['score']:.3f}")
                    report_sections.append(f"**å…³é”®ç»“è®º**: {finding['content'][:200]}...")
                    report_sections.append("")
            
            # æ¨èè¡ŒåŠ¨
            if 'recommendation' in semantic_findings:
                report_sections.append("\n## ğŸ¯ æ¨èè¡ŒåŠ¨")
                for finding in semantic_findings['recommendation']:
                    report_sections.append(f"### {finding['title']}")
                    report_sections.append(f"**ä¼˜å…ˆçº§**: {self._calculate_priority(finding['score'])}")
                    report_sections.append(f"**å»ºè®®å†…å®¹**: {finding['content'][:200]}...")
                    report_sections.append("")
            
            # å…¶ä»–ç›¸å…³åˆ†æ
            other_sections = {k: v for k, v in semantic_findings.items() 
                            if k not in ['hypothesis', 'conclusion', 'recommendation']}
            if other_sections:
                report_sections.append("\n## ğŸ“ˆ è¡¥å……åˆ†æ")
                for section_type, findings in other_sections.items():
                    report_sections.append(f"### {self._get_section_display_name(section_type)}")
                    for finding in findings[:2]:  # é™åˆ¶æ¯ä¸ªç±»å‹æœ€å¤š2ä¸ª
                        report_sections.append(f"- **{finding['title']}**: {finding['content'][:150]}...")
                    report_sections.append("")

        # æ•°æ®æ´å¯Ÿ
        if metrics_data:
            report_sections.append("\n## ğŸ“Š æ•°æ®æ´å¯Ÿ")
            for key, value in metrics_data.items():
                if isinstance(value, dict) and 'trend' in value:
                    trend_indicator = "ğŸ“ˆ" if value.get('trend', 0) > 0 else "ğŸ“‰" if value.get('trend', 0) < 0 else "â¡ï¸"
                    report_sections.append(f"- **{key}**: {value.get('current', 'N/A')} {trend_indicator}")
                else:
                    report_sections.append(f"- **{key}**: {value}")

        # æ™ºèƒ½å»ºè®®
        report_sections.append("\n## ğŸš€ æ™ºèƒ½å»ºè®®")
        suggestions = self._generate_intelligent_suggestions(rag_results, metrics_data)
        for suggestion in suggestions:
            report_sections.append(f"- {suggestion}")
        
        # ç›¸å…³æ–‡æ¡£ç´¢å¼•
        if rag_results:
            report_sections.append("\n## ğŸ“š ç›¸å…³æ–‡æ¡£ç´¢å¼•")
            for i, result in enumerate(rag_results[:5], 1):
                report_sections.append(
                    f"{i}. **{result['title']}** (ç›¸å…³åº¦: {result['score']:.3f}, "
                    f"ç±»å‹: {result['content_type']}, è¯­ä¹‰æ®µè½: {result.get('semantic_section', 'general')})"
                )

        return "\n".join(report_sections)
    
    def _get_semantic_sections_by_intent(self, intent: IntentType) -> List[str]:
        """æ ¹æ®æ„å›¾ç±»å‹é€‰æ‹©åˆé€‚çš„è¯­ä¹‰æ®µè½"""
        intent_mapping = {
            IntentType.HYPOTHESIS_ANALYSIS: ['hypothesis', 'analysis'],
            IntentType.CONCLUSION_SUMMARY: ['conclusion', 'executive_summary'],
            IntentType.RECOMMENDATION_REQUEST: ['recommendation', 'future_action'],
            IntentType.DATA_ANALYSIS: ['analysis', 'conclusion'],
            IntentType.GENERAL_INQUIRY: ['executive_summary', 'conclusion', 'recommendation']
        }
        return intent_mapping.get(intent, ['general'])
    
    def _organize_findings_by_semantic_type(self, rag_results: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """æŒ‰è¯­ä¹‰æ®µè½ç±»å‹ç»„ç»‡æœç´¢ç»“æœ"""
        organized = {}
        for result in rag_results:
            semantic_section = result.get('semantic_section', 'general')
            if semantic_section not in organized:
                organized[semantic_section] = []
            organized[semantic_section].append(result)
        
        # æŒ‰ç›¸å…³åº¦æ’åºæ¯ä¸ªç±»å‹çš„ç»“æœ
        for section_type in organized:
            organized[section_type].sort(key=lambda x: x.get('score', 0), reverse=True)
        
        return organized
    
    def _calculate_priority(self, score: float) -> str:
        """æ ¹æ®ç›¸å…³åº¦åˆ†æ•°è®¡ç®—ä¼˜å…ˆçº§"""
        if score >= 0.8:
            return "ğŸ”´ é«˜ä¼˜å…ˆçº§"
        elif score >= 0.6:
            return "ğŸŸ¡ ä¸­ä¼˜å…ˆçº§"
        else:
            return "ğŸŸ¢ ä½ä¼˜å…ˆçº§"
    
    def _get_section_display_name(self, section_type: str) -> str:
        """è·å–è¯­ä¹‰æ®µè½ç±»å‹çš„æ˜¾ç¤ºåç§°"""
        display_names = {
            'hypothesis': 'å‡è®¾åˆ†æ',
            'conclusion': 'ç»“è®ºæ€»ç»“',
            'recommendation': 'æ¨èè¡ŒåŠ¨',
            'analysis': 'æ·±åº¦åˆ†æ',
            'executive_summary': 'æ‰§è¡Œæ‘˜è¦',
            'future_action': 'æœªæ¥è¡ŒåŠ¨',
            'general': 'ä¸€èˆ¬å†…å®¹'
        }
        return display_names.get(section_type, section_type.title())
    
    def _generate_intelligent_suggestions(self, rag_results: List[Dict[str, Any]], 
                                        metrics_data: Dict[str, Any]) -> List[str]:
        """åŸºäºRAGç»“æœå’ŒæŒ‡æ ‡æ•°æ®ç”Ÿæˆæ™ºèƒ½å»ºè®®"""
        suggestions = []
        
        # åŸºäºæ–‡æ¡£æ•°é‡çš„å»ºè®®
        if len(rag_results) == 0:
            suggestions.append("å»ºè®®æ‰©å±•æœç´¢èŒƒå›´æˆ–è°ƒæ•´æŸ¥è¯¢å…³é”®è¯ä»¥è·å–æ›´å¤šç›¸å…³ä¿¡æ¯")
        elif len(rag_results) < 3:
            suggestions.append("ç›¸å…³å†å²æ¡ˆä¾‹è¾ƒå°‘ï¼Œå»ºè®®ç»“åˆå®æ—¶æ•°æ®è¿›è¡Œåˆ†æ")
        
        # åŸºäºè¯­ä¹‰æ®µè½åˆ†å¸ƒçš„å»ºè®®
        semantic_types = set(result.get('semantic_section', 'general') for result in rag_results)
        if 'recommendation' not in semantic_types:
            suggestions.append("ç¼ºå°‘å…·ä½“çš„è¡ŒåŠ¨å»ºè®®ï¼Œå»ºè®®æŸ¥æ‰¾æ›´å¤šåŒ…å«æ¨èæªæ–½çš„å†å²æ¡ˆä¾‹")
        if 'hypothesis' not in semantic_types:
            suggestions.append("å»ºè®®è¡¥å……å‡è®¾åˆ†æä»¥æ›´å¥½åœ°ç†è§£é—®é¢˜æ ¹å› ")
        
        # åŸºäºæŒ‡æ ‡æ•°æ®çš„å»ºè®®
        if metrics_data:
            trending_down = any(
                isinstance(v, dict) and v.get('trend', 0) < 0 
                for v in metrics_data.values()
            )
            if trending_down:
                suggestions.append("æ£€æµ‹åˆ°ä¸‹é™è¶‹åŠ¿ï¼Œå»ºè®®ä¼˜å…ˆå…³æ³¨é£é™©æ§åˆ¶æªæ–½")
        
        # åŸºäºæ–‡æ¡£è´¨é‡çš„å»ºè®®
        high_quality_docs = [r for r in rag_results if r.get('score', 0) > 0.7]
        if len(high_quality_docs) < len(rag_results) * 0.5:
            suggestions.append("éƒ¨åˆ†åŒ¹é…æ–‡æ¡£ç›¸å…³åº¦è¾ƒä½ï¼Œå»ºè®®ç»†åŒ–æŸ¥è¯¢æ¡ä»¶")
        
        # é»˜è®¤å»ºè®®
        if not suggestions:
            suggestions.extend([
                "å»ºè®®ç»“åˆå¤šä¸ªç»´åº¦çš„å†å²æ•°æ®è¿›è¡Œç»¼åˆåˆ†æ",
                "å…³æ³¨è¶‹åŠ¿å˜åŒ–å’Œå¼‚å¸¸æŒ‡æ ‡ï¼ŒåŠæ—¶è°ƒæ•´ç­–ç•¥",
                "å®šæœŸå›é¡¾å’Œæ›´æ–°åˆ†ææ¨¡å‹ä»¥æé«˜å‡†ç¡®æ€§"
            ])
        
        return suggestions[:5]  # é™åˆ¶å»ºè®®æ•°é‡
    
    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    async def search_by_simulation_id(self, simulation_id: str) -> List[Dict[str, Any]]:
        """æ ¹æ®ä»¿çœŸIDæœç´¢ç›¸å…³æ–‡æ¡£"""
        try:
            documents = await self.rag_service.get_challenge_by_simulation_id(simulation_id)
            
            converted_results = []
            for doc in documents:
                doc_dict = {
                    "title": doc.challenge_title,
                    "content": doc.markdown_content,
                    "document_id": doc.id,
                    "content_type": doc.content_type,
                    "simulation_id": doc.simulation_id,
                    "challenge_description": doc.challenge_description,
                    "semantic_section": doc.semantic_section,
                    "section_title": doc.section_title,
                    "token_count": doc.token_count,
                    "chunk_index": doc.chunk_index,
                    "total_chunks": doc.total_chunks,
                    "metadata": doc.metadata,
                    "created_at": doc.created_at
                }
                converted_results.append(doc_dict)
            
            return converted_results
            
        except Exception as e:
            return []
    
    async def get_database_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            return await self.rag_service.get_database_stats()
        except Exception as e:
            return {"error": str(e)}




